{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ae78f27-5928-49a6-92e3-ddade9df2de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c20f43e-df46-4722-bb0a-2f7ca1f2ed36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device type : cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "print(f\"Device type : {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff1352be-6927-4132-8102-9e52210edf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024f094c-902c-414a-a2de-ffc381235432",
   "metadata": {},
   "source": [
    "- Bert is a encoder based model, it learns bidirectional method. Both learns left and right at the same time.\n",
    "- It uses Masked Language Model\n",
    "- Word meaning is determined both left hand side and right hand side words by doing this, more comprehensive and deep meaning is captured. \n",
    "- Some use cases: QA, sentiment analysis, sentence pair classification (finding sentence similarity), NER, POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89583934-1a7d-4f2d-8a04-1007fdffe936",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained(\"bert-base-uncased\", torch_dtype=torch.float16).to(device)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2453e7fe-32a7-4986-b948-846cfef3078b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model # There is no decoder unit/section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cd545f1-c470-45ef-883f-75cdd01ae07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Hi I am working Machine Learning Deep Learning and Natural Language Processing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3adc95d3-a009-40c9-98e8-4f87acde67da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi',\n",
       " 'i',\n",
       " 'am',\n",
       " 'working',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'deep',\n",
       " 'learning',\n",
       " 'and',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "724d666f-51cf-49a3-9053-ac71b51b18c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'hi', 'i', 'am', 'working', 'machine', 'learning', 'deep', 'learning', 'and', 'natural', 'language', 'processing', '[SEP]']\n",
      "len(tokens): 14\n"
     ]
    }
   ],
   "source": [
    "tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
    "print(tokens)\n",
    "print(f\"len(tokens): {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d2275cd-595b-47bf-9353-0381f601e7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two_power: 16\n"
     ]
    }
   ],
   "source": [
    "two_power = 2\n",
    "while (two_power<len(tokens)): two_power *=2\n",
    "print(f\"two_power: {two_power}\") # find minimum two power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b758a002-31a9-43c4-bfe7-523f00166be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens : ['[CLS]', 'hi', 'i', 'am', 'working', 'machine', 'learning', 'deep', 'learning', 'and', 'natural', 'language', 'processing', '[SEP]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(tokens), two_power):\n",
    "    tokens += ['[PAD]']\n",
    "print(f\"tokens : {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94c311e7-6316-44be-b227-eb5acf27d68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get attention mask and tokens ids of given text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82d1bbfd-e3ec-4140-9e0a-0924358386f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "attention_mask = [1 if i != '[PAD]' else 0 for i in tokens]\n",
    "print(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d61f42f6-96c1-467b-8e19-e96486c7239e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_ids: [101, 7632, 1045, 2572, 2551, 3698, 4083, 2784, 4083, 1998, 3019, 2653, 6364, 102, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(f\"token_ids: {token_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c0be430-ca49-4a61-b70b-6760f918d746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add 1 dimensional to these for feeding torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bacf461-f253-4729-a2b7-f392e5e84078",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = torch.tensor(token_ids).unsqueeze(0) # add 1 dimensional for pytorch\n",
    "attention_mask = torch.tensor(attention_mask).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7fbe10-14a5-4aee-ba11-95c115b81ec8",
   "metadata": {},
   "source": [
    "<div style=\"color: white; display: block; border-radius: 5px; background-color: #09ba73; width: 100%; height: 60%; font-size: 110%; font-family: Verdana; letter-spacing: 0.5px;\">\n",
    "    <h2 style=\"padding: 10px; color: white;\">Getting the embedding</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3fff3bc-857f-4ea5-b14b-23972f455d33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Embedding which is generated from bert model:\n",
      " BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.0384,  0.3845, -0.1342,  ..., -0.2157,  0.2859,  0.4897],\n",
      "         [ 0.6211,  0.2075,  0.0930,  ..., -0.4775,  0.8149, -0.1230],\n",
      "         [ 0.4028,  0.2793, -0.1249,  ..., -0.3489, -0.1194, -0.0690],\n",
      "         ...,\n",
      "         [ 0.5371,  0.2135, -0.2998,  ...,  0.0432, -0.6787, -0.0552],\n",
      "         [ 0.0261,  0.0324,  0.2257,  ...,  0.0723, -0.2043,  0.1582],\n",
      "         [-0.0630, -0.1371,  0.2262,  ...,  0.3215, -0.1180,  0.1091]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-9.1406e-01, -5.3760e-01, -9.5459e-01,  8.7744e-01,  7.9834e-01,\n",
      "         -3.6963e-01,  8.9111e-01,  3.6401e-01, -8.2959e-01, -1.0000e+00,\n",
      "         -3.4839e-01,  9.7656e-01,  9.7754e-01,  7.0801e-01,  8.8330e-01,\n",
      "         -7.9883e-01, -6.4990e-01, -7.1387e-01,  4.3115e-01, -3.6572e-01,\n",
      "          7.5488e-01,  1.0000e+00, -2.1851e-01,  4.4946e-01,  5.2979e-01,\n",
      "          9.9268e-01, -8.4277e-01,  9.1211e-01,  9.5166e-01,  7.7539e-01,\n",
      "         -7.6611e-01,  3.5376e-01, -9.9316e-01, -3.0933e-01, -9.5654e-01,\n",
      "         -9.9365e-01,  5.8545e-01, -7.3291e-01, -9.9426e-02, -1.9821e-02,\n",
      "         -9.0771e-01,  4.9829e-01,  1.0000e+00, -4.2407e-01,  6.5771e-01,\n",
      "         -3.1909e-01, -1.0000e+00,  3.3643e-01, -9.1309e-01,  9.2773e-01,\n",
      "          8.9795e-01,  8.3008e-01,  3.5742e-01,  5.6201e-01,  5.5273e-01,\n",
      "         -3.3618e-01,  3.1616e-02,  1.8042e-01, -4.6680e-01, -6.4014e-01,\n",
      "         -7.2168e-01,  6.5283e-01, -9.1650e-01, -8.9111e-01,  9.1797e-01,\n",
      "          8.6865e-01, -3.2983e-01, -4.1138e-01, -2.6611e-01, -5.4657e-02,\n",
      "          9.0625e-01,  3.1519e-01, -1.5320e-01, -8.6475e-01,  6.3232e-01,\n",
      "          2.9053e-01, -6.4062e-01,  1.0000e+00, -7.3828e-01, -9.8145e-01,\n",
      "          9.4629e-01,  8.3301e-01,  5.4395e-01, -4.4434e-01,  5.9717e-01,\n",
      "         -1.0000e+00,  7.1582e-01, -2.3633e-01, -9.9219e-01,  2.3389e-01,\n",
      "          6.6943e-01, -2.8125e-01,  8.6133e-01,  6.2891e-01, -7.4365e-01,\n",
      "         -5.8105e-01, -3.6060e-01, -8.2959e-01, -4.2114e-01, -5.6250e-01,\n",
      "          1.9360e-01, -3.5229e-01, -5.3320e-01, -5.7617e-01,  4.1528e-01,\n",
      "         -6.8066e-01, -6.6943e-01,  5.5566e-01,  4.5923e-01,  6.9580e-01,\n",
      "          4.5386e-01, -4.2310e-01,  5.0439e-01, -9.4873e-01,  6.5186e-01,\n",
      "         -4.5410e-01, -9.9072e-01, -4.9976e-01, -9.8877e-01,  8.4473e-01,\n",
      "         -3.9600e-01, -4.2749e-01,  9.4482e-01, -4.2676e-01,  6.2109e-01,\n",
      "         -2.3340e-01, -9.2676e-01, -1.0000e+00, -7.7734e-01, -6.3818e-01,\n",
      "         -3.2593e-01, -4.3042e-01, -9.8486e-01, -9.6191e-01,  7.2705e-01,\n",
      "          9.4287e-01,  4.0259e-01,  1.0000e+00, -4.0137e-01,  9.3799e-01,\n",
      "         -3.5034e-01, -7.8906e-01,  6.6357e-01, -4.8926e-01,  8.5205e-01,\n",
      "          3.9111e-01, -7.0312e-01,  2.5879e-01, -6.8164e-01,  4.0405e-01,\n",
      "         -7.9199e-01, -2.0300e-01, -7.9492e-01, -9.1797e-01, -4.2017e-01,\n",
      "          9.2773e-01, -6.3379e-01, -9.5947e-01, -2.5000e-01, -3.2739e-01,\n",
      "         -6.1377e-01,  8.7402e-01,  8.4277e-01,  5.5908e-01, -5.1465e-01,\n",
      "          5.8203e-01,  5.1221e-01,  6.1572e-01, -8.3740e-01, -2.5000e-01,\n",
      "          5.2979e-01, -4.8413e-01, -8.9795e-01, -9.7998e-01, -4.6899e-01,\n",
      "          5.6250e-01,  9.8730e-01,  7.1777e-01,  3.9624e-01,  8.3789e-01,\n",
      "         -4.4043e-01,  7.6221e-01, -9.6631e-01,  9.8486e-01, -2.9907e-01,\n",
      "          3.5400e-01, -5.7422e-01,  6.3428e-01, -8.9844e-01,  3.1079e-01,\n",
      "          8.6035e-01, -7.3633e-01, -8.5107e-01, -2.0947e-01, -6.1377e-01,\n",
      "         -5.1562e-01, -8.7256e-01,  6.4941e-01, -4.3555e-01, -4.1138e-01,\n",
      "         -2.2681e-01,  9.2285e-01,  9.6387e-01,  8.0078e-01,  3.9575e-01,\n",
      "          6.9727e-01, -9.0674e-01, -5.6152e-01,  2.7100e-01,  2.8564e-01,\n",
      "          3.2983e-01,  9.9268e-01, -8.0273e-01, -1.9519e-01, -9.2236e-01,\n",
      "         -9.8438e-01,  7.8491e-02, -8.5449e-01, -3.6841e-01, -8.2227e-01,\n",
      "          7.1289e-01, -4.4751e-01,  4.8438e-01,  5.0342e-01, -9.6436e-01,\n",
      "         -8.0762e-01,  4.2627e-01, -5.6787e-01,  5.8447e-01, -5.3271e-01,\n",
      "          9.6973e-01,  9.3164e-01, -8.0908e-01,  7.2461e-01,  9.5850e-01,\n",
      "         -9.6631e-01, -7.8760e-01,  7.2705e-01, -3.9600e-01,  9.2334e-01,\n",
      "         -7.1826e-01,  9.9316e-01,  9.4580e-01,  8.6621e-01, -8.9844e-01,\n",
      "         -9.0967e-01, -8.9795e-01, -7.6660e-01, -1.4624e-01,  4.3335e-01,\n",
      "          9.2822e-01,  6.1328e-01,  4.9341e-01,  2.4658e-01, -6.5771e-01,\n",
      "          9.9805e-01, -9.8047e-01, -9.6094e-01, -5.3613e-01, -4.3359e-01,\n",
      "         -9.9170e-01,  9.1943e-01,  4.1821e-01,  7.6221e-01, -5.8350e-01,\n",
      "         -7.8174e-01, -9.6436e-01,  8.2275e-01,  1.9934e-01,  9.7754e-01,\n",
      "         -6.2891e-01, -9.2529e-01, -8.0371e-01, -9.3408e-01,  8.1491e-04,\n",
      "         -3.1201e-01, -5.0244e-01,  7.4425e-03, -9.4385e-01,  6.3525e-01,\n",
      "          6.9580e-01,  6.5576e-01, -8.1348e-01,  9.9902e-01,  1.0000e+00,\n",
      "          9.8145e-01,  8.7354e-01,  8.8525e-01, -1.0000e+00, -7.2070e-01,\n",
      "          1.0000e+00, -9.9658e-01, -1.0000e+00, -9.0723e-01, -7.8320e-01,\n",
      "          4.5605e-01, -1.0000e+00, -2.6489e-01, -4.5105e-02, -9.0234e-01,\n",
      "          7.7148e-01,  9.7314e-01,  9.8730e-01, -1.0000e+00,  8.2910e-01,\n",
      "          9.3262e-01, -6.4648e-01,  9.7754e-01, -5.8643e-01,  9.6387e-01,\n",
      "          6.6797e-01,  7.4902e-01, -3.2178e-01,  5.4883e-01, -9.6973e-01,\n",
      "         -8.9258e-01, -6.5137e-01, -8.5645e-01,  9.9951e-01,  2.9614e-01,\n",
      "         -8.5986e-01, -8.9258e-01,  6.4404e-01, -2.8931e-01, -5.3528e-02,\n",
      "         -9.7217e-01, -3.5522e-01,  4.7021e-01,  8.7402e-01,  2.9199e-01,\n",
      "          4.5728e-01, -7.0605e-01,  4.0698e-01,  1.8323e-01,  4.7192e-01,\n",
      "          6.2451e-01, -9.2773e-01, -6.0840e-01,  3.6841e-01, -8.5571e-02,\n",
      "         -7.5537e-01, -9.6484e-01,  9.6631e-01, -3.7769e-01,  7.7197e-01,\n",
      "          1.0000e+00,  2.3108e-01, -9.1260e-01,  7.6562e-01,  3.9014e-01,\n",
      "         -4.3750e-01,  1.0000e+00,  9.1553e-01, -9.8340e-01, -5.6982e-01,\n",
      "          7.4658e-01, -7.3730e-01, -7.3877e-01,  9.9951e-01, -4.2700e-01,\n",
      "         -7.4170e-01, -5.2295e-01,  9.8633e-01, -9.9170e-01,  9.9854e-01,\n",
      "         -9.1797e-01, -9.6338e-01,  9.5898e-01,  9.3018e-01, -7.5977e-01,\n",
      "         -7.3926e-01,  2.9614e-01, -7.9541e-01,  4.2798e-01, -9.2969e-01,\n",
      "          7.3535e-01,  5.5615e-01, -2.7905e-01,  9.0283e-01, -8.2471e-01,\n",
      "         -5.2881e-01,  4.4653e-01, -6.4795e-01, -9.1309e-02,  9.6436e-01,\n",
      "          5.2783e-01, -3.9966e-01,  1.7358e-01, -4.9048e-01, -8.1104e-01,\n",
      "         -9.6045e-01,  6.6455e-01,  1.0000e+00, -4.4653e-01,  8.5254e-01,\n",
      "         -6.2939e-01, -1.1456e-01,  4.3602e-03,  6.3818e-01,  6.6846e-01,\n",
      "         -4.5044e-01, -8.9648e-01,  6.3623e-01, -9.4971e-01, -9.9072e-01,\n",
      "          6.3623e-01,  2.7710e-01, -5.3027e-01,  1.0000e+00,  5.6104e-01,\n",
      "          3.3960e-01,  4.1309e-01,  9.8486e-01,  1.8445e-01,  6.4502e-01,\n",
      "          9.1553e-01,  9.8340e-01, -3.8892e-01,  6.2988e-01,  8.5791e-01,\n",
      "         -9.0039e-01, -5.2393e-01, -7.0020e-01,  1.6064e-01, -9.4238e-01,\n",
      "         -4.9957e-02, -9.6973e-01,  9.5850e-01,  9.4629e-01,  3.9307e-01,\n",
      "          4.2212e-01,  8.4961e-01,  1.0000e+00, -8.2666e-01,  6.9287e-01,\n",
      "         -1.4282e-01,  7.2510e-01, -1.0000e+00, -7.7002e-01, -4.9854e-01,\n",
      "         -2.6709e-01, -8.8428e-01, -5.4590e-01,  2.8345e-01, -9.7656e-01,\n",
      "          8.5205e-01,  6.4941e-01, -9.7705e-01, -9.8389e-01, -4.0820e-01,\n",
      "          9.1699e-01,  1.4990e-01, -9.9268e-01, -7.6465e-01, -5.9912e-01,\n",
      "          6.8115e-01, -4.4116e-01, -9.2090e-01, -2.6367e-01, -4.0234e-01,\n",
      "          5.0293e-01, -3.4229e-01,  6.0400e-01,  9.0430e-01,  5.3369e-01,\n",
      "         -8.6035e-01, -5.5078e-01, -2.2583e-01, -8.5889e-01,  8.6475e-01,\n",
      "         -8.5254e-01, -9.4385e-01, -2.3608e-01,  1.0000e+00, -5.0195e-01,\n",
      "          9.4824e-01,  7.2168e-01,  7.6465e-01, -3.5522e-01,  3.6694e-01,\n",
      "          9.6680e-01,  6.0156e-01, -8.7256e-01, -8.1787e-01, -3.5034e-01,\n",
      "         -5.4590e-01,  7.8271e-01,  6.9678e-01,  8.2129e-01,  8.2568e-01,\n",
      "          8.6914e-01,  4.7046e-01, -2.2583e-01,  2.7148e-01,  9.9951e-01,\n",
      "         -3.9478e-01, -3.8721e-01, -5.1562e-01, -3.0762e-01, -5.6689e-01,\n",
      "         -4.1675e-01,  1.0000e+00,  4.2969e-01,  5.8643e-01, -9.9072e-01,\n",
      "         -9.0137e-01, -8.8867e-01,  1.0000e+00,  8.8281e-01, -8.0322e-01,\n",
      "          7.8760e-01,  7.3438e-01, -2.3083e-01,  8.2812e-01, -2.6587e-01,\n",
      "         -3.5571e-01,  4.0796e-01,  2.2131e-01,  9.5947e-01, -6.5137e-01,\n",
      "         -9.7412e-01, -7.2998e-01,  5.0391e-01, -9.6387e-01,  1.0000e+00,\n",
      "         -7.1387e-01, -5.2246e-01, -5.7666e-01, -4.5215e-01, -4.3365e-02,\n",
      "         -1.3481e-02, -9.7949e-01, -3.5254e-01,  3.7939e-01,  9.6143e-01,\n",
      "          4.2773e-01, -6.1279e-01, -8.7305e-01,  7.7734e-01,  8.4424e-01,\n",
      "         -9.4336e-01, -9.1406e-01,  9.7070e-01, -9.6826e-01,  5.9668e-01,\n",
      "          1.0000e+00,  5.1221e-01,  2.6367e-01,  4.3652e-01, -6.8018e-01,\n",
      "          4.1040e-01, -5.6152e-01,  7.0654e-01, -9.5605e-01, -5.4590e-01,\n",
      "         -3.5986e-01,  4.1504e-01, -2.0386e-01, -7.7979e-01,  6.4453e-01,\n",
      "          3.1689e-01, -5.3027e-01, -7.2363e-01, -2.6855e-01,  5.0098e-01,\n",
      "          8.3203e-01, -3.4204e-01, -1.8262e-01,  1.9055e-01, -3.3154e-01,\n",
      "         -9.0967e-01, -5.1172e-01, -5.6982e-01, -1.0000e+00,  7.5537e-01,\n",
      "         -1.0000e+00,  4.6631e-01,  3.2422e-01, -3.0835e-01,  8.2910e-01,\n",
      "          7.8516e-01,  6.1719e-01, -7.0850e-01, -8.7891e-01,  2.9785e-01,\n",
      "          7.1777e-01, -4.6753e-01, -2.4634e-01, -6.5479e-01,  4.5142e-01,\n",
      "         -2.7759e-01,  3.7085e-01, -6.7969e-01,  7.6660e-01, -4.0820e-01,\n",
      "          1.0000e+00,  3.1787e-01, -8.0127e-01, -9.6680e-01,  3.7866e-01,\n",
      "         -4.9634e-01,  1.0000e+00, -8.9648e-01, -9.6533e-01,  5.4150e-01,\n",
      "         -8.4375e-01, -8.6328e-01,  4.6289e-01,  2.9688e-01, -8.4668e-01,\n",
      "         -9.7070e-01,  8.7646e-01,  9.0283e-01, -5.6445e-01,  6.3232e-01,\n",
      "         -4.6094e-01, -7.2803e-01,  5.9357e-02,  9.1602e-01,  9.9170e-01,\n",
      "          4.6436e-01,  9.2529e-01, -2.7954e-01, -4.6484e-01,  9.6191e-01,\n",
      "          4.8218e-01,  4.4141e-01,  2.3840e-01,  1.0000e+00,  4.8071e-01,\n",
      "         -9.4385e-01, -1.2177e-01, -9.8291e-01, -3.9185e-01, -9.5654e-01,\n",
      "          5.1807e-01,  4.3262e-01,  8.9258e-01, -3.3057e-01,  9.6094e-01,\n",
      "         -8.1982e-01,  1.4636e-01, -5.7471e-01, -6.2744e-01,  4.8120e-01,\n",
      "         -9.3555e-01, -9.8730e-01, -9.7900e-01,  5.8252e-01, -5.2002e-01,\n",
      "         -1.5271e-01,  4.0796e-01,  1.7346e-01,  5.4053e-01,  4.8193e-01,\n",
      "         -1.0000e+00,  9.2529e-01,  5.1758e-01,  9.4385e-01,  9.6777e-01,\n",
      "          8.2715e-01,  6.0791e-01,  3.5913e-01, -9.8486e-01, -9.6143e-01,\n",
      "         -5.5615e-01, -3.6914e-01,  7.7393e-01,  7.6709e-01,  8.9648e-01,\n",
      "          5.5273e-01, -5.4248e-01, -6.7676e-01, -7.3438e-01, -9.1602e-01,\n",
      "         -9.9365e-01,  6.5820e-01, -5.7666e-01, -9.5605e-01,  9.7314e-01,\n",
      "         -1.0590e-01, -2.4463e-01, -1.8713e-01, -8.8037e-01,  8.7939e-01,\n",
      "          8.0371e-01,  5.5811e-01,  2.0264e-01,  6.4746e-01,  8.9014e-01,\n",
      "          9.5264e-01,  9.7998e-01, -9.1992e-01,  8.1348e-01, -7.5049e-01,\n",
      "          5.2539e-01,  8.9355e-01, -9.5361e-01,  2.6172e-01,  6.8799e-01,\n",
      "         -5.2783e-01,  4.7290e-01, -3.5205e-01, -9.4727e-01,  6.1084e-01,\n",
      "         -4.9048e-01,  7.5586e-01, -5.1318e-01, -9.0759e-02, -4.8608e-01,\n",
      "         -1.5735e-01, -7.6904e-01, -7.4512e-01,  6.3037e-01,  6.1572e-01,\n",
      "          9.2969e-01,  9.2969e-01, -1.7334e-01, -6.8896e-01, -2.1863e-01,\n",
      "         -8.7695e-01, -9.4238e-01,  8.8623e-01, -2.3120e-01, -6.0693e-01,\n",
      "          8.6377e-01,  1.6760e-01,  8.2129e-01,  4.3164e-01, -5.1367e-01,\n",
      "         -4.9487e-01, -8.6572e-01,  8.8770e-01, -7.7051e-01, -6.5625e-01,\n",
      "         -7.0752e-01,  8.0029e-01,  4.4531e-01,  1.0000e+00, -9.0186e-01,\n",
      "         -9.3359e-01, -6.7725e-01, -5.6250e-01,  4.8267e-01, -6.0938e-01,\n",
      "         -1.0000e+00,  4.7998e-01, -6.6016e-01,  8.3398e-01, -6.1719e-01,\n",
      "          9.3994e-01, -7.0312e-01, -9.6191e-01, -4.5215e-01,  7.9199e-01,\n",
      "          8.3447e-01, -6.0156e-01, -6.9238e-01,  5.6201e-01,  1.9983e-01,\n",
      "          9.8340e-01,  8.6133e-01, -7.0862e-02, -1.2817e-01,  6.8164e-01,\n",
      "         -9.0674e-01, -7.5781e-01,  9.2285e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n"
     ]
    }
   ],
   "source": [
    "embedding = model(token_ids.cuda(), attention_mask = attention_mask.cuda())\n",
    "print(f\"The Embedding which is generated from bert model:\\n {embedding}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "938108f9-cde3-4665-9353-a12e0083df1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 768])\n"
     ]
    }
   ],
   "source": [
    "print(embedding[0].shape) # 1 is added above with unsqueeze, 16 is num of token ids, 768 is feature vector size for each token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fa87eb-c7c4-46e2-a7c9-0411b67ed1b6",
   "metadata": {},
   "source": [
    "<div style=\"color: white; display: block; border-radius: 5px; background-color: #09ba73; width: 100%; height: 60%; font-size: 110%; font-family: Verdana; letter-spacing: 0.5px;\">\n",
    "    <h2 style=\"padding: 10px; color: white;\">More basic and fast way below</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc99353e-75b3-4908-b0bc-dbab3fad4197",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output : BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.1469,  0.4968, -0.0965,  ..., -0.3440,  0.4072,  0.3835],\n",
      "         [ 1.1006,  0.5830,  0.3108,  ..., -0.3135,  1.0469, -0.4131],\n",
      "         [ 0.4285, -0.0618, -0.3525,  ..., -0.4912,  0.2061,  0.1394],\n",
      "         ...,\n",
      "         [-0.3845,  0.4897,  0.2377,  ..., -0.6528,  0.0277, -0.3875],\n",
      "         [ 0.3494,  0.0120, -0.6250,  ..., -0.4351,  0.1409, -0.3704],\n",
      "         [ 0.6099,  0.1139, -0.3408,  ...,  0.1274, -0.3647, -0.2844]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.7905, -0.3721, -0.6030,  0.6353,  0.5542, -0.0818,  0.7104,  0.2168,\n",
      "         -0.2081, -1.0000, -0.1654,  0.8662,  0.9756,  0.1624,  0.8545, -0.5820,\n",
      "         -0.2283, -0.5332,  0.2202, -0.0406,  0.5293,  0.9995,  0.3635,  0.2527,\n",
      "          0.2622,  0.9248, -0.6997,  0.8774,  0.9370,  0.6680, -0.5293,  0.1333,\n",
      "         -0.9883, -0.0666, -0.6631, -0.9854,  0.3103, -0.6777,  0.0499,  0.1372,\n",
      "         -0.8726,  0.3320,  0.9995, -0.6011,  0.3770, -0.2185, -1.0000,  0.1288,\n",
      "         -0.8687,  0.4021,  0.5571,  0.3174,  0.1475,  0.3081,  0.3716,  0.0381,\n",
      "         -0.1984,  0.0765, -0.2698, -0.4341, -0.6304,  0.4414, -0.6348, -0.8320,\n",
      "          0.5552,  0.3889, -0.0604, -0.1406,  0.0087, -0.2397,  0.8042,  0.1355,\n",
      "          0.2583, -0.8174, -0.0704,  0.0961, -0.5527,  1.0000, -0.4077, -0.9751,\n",
      "          0.6558,  0.3140,  0.3806,  0.2815, -0.1099, -1.0000,  0.4097, -0.1022,\n",
      "         -0.9893,  0.0718,  0.5562, -0.1475,  0.3804,  0.4495, -0.3384, -0.3369,\n",
      "         -0.1913, -0.5845, -0.1998, -0.1864,  0.0026, -0.0847, -0.2742, -0.3220,\n",
      "          0.0309, -0.4177, -0.3596,  0.3157, -0.1115,  0.5444,  0.3655, -0.2815,\n",
      "          0.1846, -0.9243,  0.4827, -0.2791, -0.9810, -0.3721, -0.9854,  0.6704,\n",
      "         -0.2388, -0.1473,  0.9243,  0.3469,  0.3503,  0.0481, -0.5220, -1.0000,\n",
      "         -0.4758, -0.5415,  0.1145, -0.1185, -0.9756, -0.9585,  0.5200,  0.9229,\n",
      "          0.1350,  0.9985, -0.1305,  0.9209,  0.1523, -0.4868,  0.1299, -0.3396,\n",
      "          0.7036,  0.1422, -0.4404,  0.1233, -0.3545, -0.0384, -0.5010, -0.0751,\n",
      "         -0.3079, -0.9155, -0.2910,  0.9136, -0.3801, -0.6099,  0.3999, -0.1131,\n",
      "         -0.4304,  0.7749,  0.6714,  0.2703, -0.1926,  0.3218, -0.1224,  0.4509,\n",
      "         -0.7539,  0.2490,  0.3481, -0.2399, -0.5449, -0.9741, -0.2172,  0.3071,\n",
      "          0.9829,  0.6108,  0.1821,  0.3906, -0.2556,  0.4392, -0.9512,  0.9771,\n",
      "         -0.0943,  0.2393, -0.4106,  0.2302, -0.8252, -0.1761,  0.6895, -0.4270,\n",
      "         -0.7251,  0.0355, -0.4709, -0.2351, -0.6172,  0.2739, -0.1970, -0.3262,\n",
      "         -0.0500,  0.9106,  0.8794,  0.5537, -0.2333,  0.5444, -0.8457, -0.3298,\n",
      "          0.0159,  0.1449,  0.0329,  0.9873, -0.7002, -0.0186, -0.8833, -0.9810,\n",
      "         -0.1360, -0.8159, -0.0939, -0.6763,  0.3889, -0.1521, -0.0397,  0.3376,\n",
      "         -0.9106, -0.6426,  0.2668, -0.3982,  0.3950, -0.2881,  0.8853,  0.6851,\n",
      "         -0.5127,  0.3533,  0.9355, -0.7988, -0.7524,  0.5630, -0.1440,  0.8037,\n",
      "         -0.5361,  0.9653,  0.7852,  0.7192, -0.8657, -0.4893, -0.7588, -0.2529,\n",
      "          0.0363, -0.2620,  0.4885,  0.4443,  0.2749,  0.6621, -0.4163,  0.9673,\n",
      "         -0.9146, -0.9487, -0.6445,  0.0533, -0.9868,  0.6226,  0.2817,  0.3711,\n",
      "         -0.3108, -0.6123, -0.9531,  0.6074,  0.0609,  0.9238, -0.2905, -0.8315,\n",
      "         -0.3354, -0.9067, -0.2120, -0.0912,  0.2156, -0.1370, -0.9277,  0.4199,\n",
      "          0.5596,  0.4739, -0.1625,  0.9863,  1.0000,  0.9619,  0.8389,  0.7808,\n",
      "         -0.9971, -0.5972,  1.0000, -0.9541, -1.0000, -0.8911, -0.5605,  0.2751,\n",
      "         -1.0000, -0.0869,  0.0570, -0.8813,  0.2262,  0.9604,  0.9487, -1.0000,\n",
      "          0.6836,  0.8716, -0.5098,  0.8770, -0.3838,  0.9595,  0.4546,  0.4897,\n",
      "         -0.1562,  0.4456, -0.8335, -0.7939, -0.0692, -0.5034,  0.9888,  0.0228,\n",
      "         -0.7104, -0.8159,  0.4382,  0.0215, -0.3259, -0.9614, -0.1882, -0.0517,\n",
      "          0.7437,  0.0199,  0.2422, -0.5347,  0.1649, -0.3572,  0.1733,  0.5601,\n",
      "         -0.8853, -0.2859,  0.2118, -0.4661, -0.1647, -0.9546,  0.9434, -0.1606,\n",
      "          0.4268,  1.0000, -0.0235, -0.8071,  0.6138,  0.1306, -0.4661,  1.0000,\n",
      "          0.6777, -0.9741, -0.4675,  0.4709, -0.3801, -0.4597,  0.9971, -0.1069,\n",
      "         -0.1020, -0.0690,  0.9839, -0.9863,  0.9800, -0.8608, -0.9497,  0.9507,\n",
      "          0.9170, -0.5571, -0.5928, -0.0057, -0.1527,  0.1771, -0.8535,  0.5547,\n",
      "          0.2952, -0.0251,  0.8516, -0.5938, -0.4568,  0.2605, -0.3215,  0.1384,\n",
      "          0.7754,  0.3801, -0.2090, -0.0626, -0.1614, -0.7217, -0.9492,  0.4023,\n",
      "          1.0000, -0.0310,  0.5845, -0.1925,  0.0220, -0.3179,  0.3362,  0.4578,\n",
      "         -0.2146, -0.7432,  0.3025, -0.8521, -0.9863,  0.4578,  0.1160, -0.2031,\n",
      "          0.9995,  0.3401,  0.1635, -0.0027,  0.8921,  0.0470,  0.4644,  0.3557,\n",
      "          0.9751, -0.2583,  0.4807,  0.6060, -0.4446, -0.3157, -0.5645, -0.0612,\n",
      "         -0.9365,  0.1012, -0.9355,  0.9414,  0.6934,  0.2280,  0.1851,  0.5327,\n",
      "          1.0000, -0.7598,  0.5161,  0.1526,  0.5542, -0.9976, -0.5908, -0.2905,\n",
      "         -0.0606, -0.3433, -0.3044,  0.0661, -0.9575,  0.3367,  0.1652, -0.9297,\n",
      "         -0.9766,  0.2073,  0.7246, -0.0682, -0.9302, -0.5391, -0.5332,  0.3770,\n",
      "         -0.0920, -0.9116,  0.3855, -0.2194,  0.2749, -0.1349,  0.5293,  0.3982,\n",
      "          0.7993, -0.5591, -0.2551, -0.0702, -0.6934,  0.7544, -0.6450, -0.6924,\n",
      "         -0.1011,  1.0000, -0.4458,  0.6230,  0.5430,  0.4507, -0.1126,  0.2231,\n",
      "          0.7837,  0.3215, -0.2335, -0.2549,  0.1772, -0.2413,  0.5449,  0.3252,\n",
      "          0.2241,  0.7549,  0.7632,  0.1631,  0.0361,  0.0115,  0.9893, -0.0668,\n",
      "         -0.0217, -0.1936, -0.0595, -0.3264, -0.1085,  1.0000,  0.2622,  0.2285,\n",
      "         -0.9883, -0.6499, -0.7847,  1.0000,  0.7803, -0.6582,  0.5361,  0.4883,\n",
      "         -0.1123,  0.5742, -0.1311, -0.2094,  0.2043,  0.0104,  0.9404, -0.5024,\n",
      "         -0.9639, -0.5728,  0.2157, -0.9478,  0.9990, -0.4580, -0.2910, -0.4202,\n",
      "          0.0507, -0.3838, -0.1416, -0.9683, -0.1259,  0.1069,  0.9390,  0.1046,\n",
      "         -0.4663, -0.7627,  0.2347,  0.5254, -0.6201, -0.9160,  0.9629, -0.9517,\n",
      "          0.3479,  1.0000,  0.3376, -0.3342,  0.1656, -0.3093,  0.1558, -0.1954,\n",
      "          0.5737, -0.9429, -0.3013, -0.1240,  0.1661,  0.0088, -0.2461,  0.5596,\n",
      "          0.1718, -0.4141, -0.5449,  0.0174,  0.2710,  0.6743, -0.1698, -0.0131,\n",
      "         -0.0140, -0.0682, -0.8027, -0.2249, -0.2428, -1.0000,  0.5107, -1.0000,\n",
      "          0.0847, -0.3049, -0.1125,  0.7671,  0.5801,  0.4436, -0.6460, -0.2467,\n",
      "          0.7290,  0.6191, -0.2072,  0.1703, -0.5649,  0.1879, -0.1056,  0.1793,\n",
      "         -0.1858,  0.6904, -0.1001,  1.0000,  0.1146, -0.4651, -0.9048,  0.1394,\n",
      "         -0.2139,  1.0000, -0.6660, -0.9448,  0.3108, -0.6284, -0.7769,  0.1899,\n",
      "          0.0214, -0.7065, -0.8208,  0.8628,  0.5630, -0.5146,  0.4456, -0.2429,\n",
      "         -0.4602, -0.0825,  0.5796,  0.9863,  0.3848,  0.8247,  0.2844, -0.2367,\n",
      "          0.9619,  0.1707, -0.0651, -0.0142,  1.0000,  0.3120, -0.9180,  0.1287,\n",
      "         -0.9624, -0.1436, -0.8867,  0.2544,  0.0960,  0.8252, -0.0936,  0.9165,\n",
      "         -0.1902, -0.0944, -0.1438, -0.0658,  0.2771, -0.8911, -0.9800, -0.9785,\n",
      "          0.3718, -0.3826,  0.1218,  0.1671, -0.0482,  0.2874,  0.2893, -1.0000,\n",
      "          0.8887,  0.2590,  0.5010,  0.9580,  0.5269,  0.3411,  0.2035, -0.9795,\n",
      "         -0.8804, -0.2810, -0.1909,  0.6113,  0.5176,  0.8564,  0.2776, -0.3979,\n",
      "         -0.4387, -0.0359, -0.7598, -0.9897,  0.4841,  0.0749, -0.8428,  0.9497,\n",
      "         -0.3777, -0.0311,  0.3523, -0.5176,  0.6895,  0.6401,  0.3076,  0.0181,\n",
      "          0.3862,  0.8115,  0.8716,  0.9785, -0.5503,  0.6450, -0.2362,  0.3140,\n",
      "          0.8623, -0.9263,  0.0897,  0.3486, -0.0699,  0.2031, -0.1787, -0.8745,\n",
      "          0.5410, -0.2551,  0.5376, -0.3337,  0.1288, -0.3127, -0.0289, -0.6938,\n",
      "         -0.6133,  0.5352,  0.3718,  0.8882,  0.7856,  0.0419, -0.5044,  0.0177,\n",
      "         -0.3362, -0.9214,  0.8184,  0.0616,  0.1959,  0.6338, -0.1196,  0.8643,\n",
      "         -0.0067, -0.2028, -0.2051, -0.6069,  0.6968, -0.5981, -0.4485, -0.4758,\n",
      "          0.6211,  0.2637,  0.9995, -0.4302, -0.4912, -0.4033, -0.2549,  0.3730,\n",
      "         -0.4033, -1.0000,  0.2186, -0.0482,  0.5469, -0.2125,  0.7378, -0.3081,\n",
      "         -0.9263, -0.2729,  0.5435,  0.4395, -0.4326, -0.2959,  0.4363,  0.6001,\n",
      "          0.8716,  0.7979,  0.0649,  0.4177,  0.5381, -0.6616, -0.5469,  0.8716]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n"
     ]
    }
   ],
   "source": [
    "text = \"Hi I am a computer engineer\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt').to(device)\n",
    "output = model(**encoded_input)\n",
    "print(f\"output : {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268f0963-14ef-4660-8fe0-1880fd232041",
   "metadata": {},
   "source": [
    "<div style=\"color: white; display: block; border-radius: 5px; background-color: #09ba73; width: 100%; height: 60%; font-size: 110%; font-family: Verdana; letter-spacing: 0.5px;\">\n",
    "    <h2 style=\"padding: 10px; color: white;\">Fill in the blank([MASK])</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc946dc6-508f-44f7-a0d3-526c15681d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.09747566282749176,\n",
       "  'token': 10533,\n",
       "  'token_str': 'carpenter',\n",
       "  'sequence': 'the man worked as a carpenter.'},\n",
       " {'score': 0.052383266389369965,\n",
       "  'token': 15610,\n",
       "  'token_str': 'waiter',\n",
       "  'sequence': 'the man worked as a waiter.'},\n",
       " {'score': 0.04962713271379471,\n",
       "  'token': 13362,\n",
       "  'token_str': 'barber',\n",
       "  'sequence': 'the man worked as a barber.'},\n",
       " {'score': 0.03788609057664871,\n",
       "  'token': 15893,\n",
       "  'token_str': 'mechanic',\n",
       "  'sequence': 'the man worked as a mechanic.'},\n",
       " {'score': 0.03768087923526764,\n",
       "  'token': 18968,\n",
       "  'token_str': 'salesman',\n",
       "  'sequence': 'the man worked as a salesman.'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
    "unmasker(\"The man worked as a [MASK].\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "090d341b-05b3-493a-ace5-05b4bf468a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From high to low scored predictions are listed as : \n",
      "carpenter\n",
      "waiter\n",
      "barber\n",
      "mechanic\n",
      "salesman\n"
     ]
    }
   ],
   "source": [
    "predictions = unmasker(\"The man worked as a [MASK].\")\n",
    "print(\"From high to low scored predictions are listed as : \")\n",
    "for i in predictions:\n",
    "    print(i[\"token_str\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "810ab618-a564-44c2-b3a7-c97ec0532ca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.2198147177696228,\n",
       "  'token': 6821,\n",
       "  'token_str': 'nurse',\n",
       "  'sequence': 'the woman worked as a nurse.'},\n",
       " {'score': 0.15974149107933044,\n",
       "  'token': 13877,\n",
       "  'token_str': 'waitress',\n",
       "  'sequence': 'the woman worked as a waitress.'},\n",
       " {'score': 0.11547322571277618,\n",
       "  'token': 10850,\n",
       "  'token_str': 'maid',\n",
       "  'sequence': 'the woman worked as a maid.'},\n",
       " {'score': 0.03796886280179024,\n",
       "  'token': 19215,\n",
       "  'token_str': 'prostitute',\n",
       "  'sequence': 'the woman worked as a prostitute.'},\n",
       " {'score': 0.030423881486058235,\n",
       "  'token': 5660,\n",
       "  'token_str': 'cook',\n",
       "  'sequence': 'the woman worked as a cook.'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker(\"The woman worked as a [MASK].\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75b58247-4bcf-4926-8789-b36c81e178a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From high to low scored predictions are listed as : \n",
      "nurse\n",
      "waitress\n",
      "maid\n",
      "prostitute\n",
      "cook\n"
     ]
    }
   ],
   "source": [
    "predictions = unmasker(\"The woman worked as a [MASK].\")\n",
    "print(\"From high to low scored predictions are listed as : \")\n",
    "for i in predictions:\n",
    "    print(i[\"token_str\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd82f02-04b2-4c91-8c37-379e35fd351b",
   "metadata": {},
   "source": [
    "<div style=\"color: white; display: block; border-radius: 5px; background-color: #09ba73; width: 100%; height: 60%; font-size: 110%; font-family: Verdana; letter-spacing: 0.5px;\">\n",
    "    <h4 style=\"padding: 10px; color: white;\">Roberta sentiment</h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "705c1acc-db4e-4064-aac8-c186cc2eca73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, logging\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "roberta_model_name = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "roberta_tokenizer_sentiment = RobertaTokenizer.from_pretrained(roberta_model_name)\n",
    "roberta_model_sentiment = RobertaForSequenceClassification.from_pretrained(roberta_model_name) # , num_labels=2\n",
    "roberta_sentiment_analyzer = pipeline(\"sentiment-analysis\", model=roberta_model_sentiment, tokenizer=roberta_tokenizer_sentiment, device=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651caec3-7e15-4ef5-bc61-280744d6af4d",
   "metadata": {},
   "source": [
    "<div style=\"color: white; display: block; border-radius: 5px; background-color: #09ba73; width: 100%; height: 60%; font-size: 110%; font-family: Verdana; letter-spacing: 0.5px;\">\n",
    "    <h4 style=\"padding: 10px; color: white;\">Bert sentiment</h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "35af5886-7a5a-4705-8b84-643dd184d8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "# sentiment analysis pipeline\n",
    "bert_model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "bert_tokenizer_sentiment = BertTokenizer.from_pretrained(bert_model_name)\n",
    "bert_model_sentiment = BertForSequenceClassification.from_pretrained(bert_model_name, ignore_mismatched_sizes=True)\n",
    "bert_sentiment_analyzer = pipeline(task='sentiment-analysis', model=bert_model_sentiment, tokenizer=bert_tokenizer_sentiment, device=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7fecb387-9a05-45ab-91df-76cbf9a326c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_list = [\"I'm not sure you're being honest.\",\n",
    "\"I'm sure you're being honest.\",\n",
    "\"I love you.\",\n",
    "\"I like you\",\n",
    "\"I hate you\",\n",
    "\"You are disgusting\",\n",
    "\"I am neutral about your attitude.\",\n",
    "\"Today, I am feeling depressed.\",\n",
    "\"Today, I am feeling nothing.\",\n",
    "\"Today, I am feeling anything.\",\n",
    "\"Today, I went to grocery.\",\n",
    "\"Today, I went to grocery to buy sugar.\",\n",
    "\"Today, I went to the grocery store with notr employees.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc30545e-5839-4706-8095-35150958c109",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def print_sentiment_df(sentence_list:list, analyzer:pipeline)->dict:\n",
    "    if(sentence_list == None or analyzer == None):\n",
    "        return None\n",
    "  \n",
    "    result = {}\n",
    "    class_name = re.findall(r'[A-Za-z]+', str(type(analyzer.model.base_model)).split('.')[-1])[0]\n",
    "    if (type(analyzer.model) == BertForSequenceClassification):\n",
    "        result[class_name]= []\n",
    "        for i in sentence_list:\n",
    "            if (sentence != None and sentence != \"\" and type(sentence)==str):\n",
    "                index = int(analyzer(i)[0][\"label\"].split()[0])\n",
    "                sentiment_map = {1: \"Very Negative\", 2: \"Negative\", 3: \"Neutral\", 4: \"Positive\", 5: \"Very Positive\"}\n",
    "                score = analyzer(i)[0][\"score\"]\n",
    "                new_row = {'sentence': i, 'sentiment': sentiment_map[index], \"score\": score}\n",
    "                result[class_name].append(new_row)\n",
    "    elif(type(analyzer.model) == RobertaForSequenceClassification):\n",
    "        result[class_name]= []\n",
    "        for i in sentence_list:\n",
    "            if (sentence != None and sentence != \"\" and type(sentence)==str):\n",
    "                sentiment = analyzer(i)[0][\"label\"]\n",
    "                score = analyzer(i)[0][\"score\"]\n",
    "                new_row = {'sentence': i, 'sentiment':sentiment, \"score\":score}\n",
    "                result[class_name].append(new_row)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "91586f52-28ed-456f-807c-12bba4f917ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llm_name</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"13\" valign=\"top\">bert</th>\n",
       "      <th>0</th>\n",
       "      <td>I'm not sure you're being honest.</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.477720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm sure you're being honest.</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.351202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I love you.</td>\n",
       "      <td>Very Positive</td>\n",
       "      <td>0.871876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I like you</td>\n",
       "      <td>Very Positive</td>\n",
       "      <td>0.474995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I hate you</td>\n",
       "      <td>Very Negative</td>\n",
       "      <td>0.634607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>You are disgusting</td>\n",
       "      <td>Very Negative</td>\n",
       "      <td>0.744359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I am neutral about your attitude.</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.348305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Today, I am feeling depressed.</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0.460997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Today, I am feeling nothing.</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0.346679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Today, I am feeling anything.</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.276451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Today, I went to grocery.</td>\n",
       "      <td>Very Positive</td>\n",
       "      <td>0.303925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Today, I went to grocery to buy sugar.</td>\n",
       "      <td>Very Positive</td>\n",
       "      <td>0.354398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Today, I went to the grocery store with notr e...</td>\n",
       "      <td>Very Negative</td>\n",
       "      <td>0.289577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"13\" valign=\"top\">roberta</th>\n",
       "      <th>0</th>\n",
       "      <td>I'm not sure you're being honest.</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.633547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm sure you're being honest.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.625573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I love you.</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.928684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I like you</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.654007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I hate you</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.786693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>You are disgusting</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.886169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I am neutral about your attitude.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.523557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Today, I am feeling depressed.</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.899095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Today, I am feeling nothing.</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.685357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Today, I am feeling anything.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.633503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Today, I went to grocery.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.612150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Today, I went to grocery to buy sugar.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.738948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Today, I went to the grocery store with notr e...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.804584</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                               sentence  \\\n",
       "llm_name sentence_id                                                      \n",
       "bert     0                            I'm not sure you're being honest.   \n",
       "         1                                I'm sure you're being honest.   \n",
       "         2                                                  I love you.   \n",
       "         3                                                   I like you   \n",
       "         4                                                   I hate you   \n",
       "         5                                           You are disgusting   \n",
       "         6                            I am neutral about your attitude.   \n",
       "         7                               Today, I am feeling depressed.   \n",
       "         8                                 Today, I am feeling nothing.   \n",
       "         9                                Today, I am feeling anything.   \n",
       "         10                                   Today, I went to grocery.   \n",
       "         11                      Today, I went to grocery to buy sugar.   \n",
       "         12           Today, I went to the grocery store with notr e...   \n",
       "roberta  0                            I'm not sure you're being honest.   \n",
       "         1                                I'm sure you're being honest.   \n",
       "         2                                                  I love you.   \n",
       "         3                                                   I like you   \n",
       "         4                                                   I hate you   \n",
       "         5                                           You are disgusting   \n",
       "         6                            I am neutral about your attitude.   \n",
       "         7                               Today, I am feeling depressed.   \n",
       "         8                                 Today, I am feeling nothing.   \n",
       "         9                                Today, I am feeling anything.   \n",
       "         10                                   Today, I went to grocery.   \n",
       "         11                      Today, I went to grocery to buy sugar.   \n",
       "         12           Today, I went to the grocery store with notr e...   \n",
       "\n",
       "                          sentiment     score  \n",
       "llm_name sentence_id                           \n",
       "bert     0                  Neutral  0.477720  \n",
       "         1                  Neutral  0.351202  \n",
       "         2            Very Positive  0.871876  \n",
       "         3            Very Positive  0.474995  \n",
       "         4            Very Negative  0.634607  \n",
       "         5            Very Negative  0.744359  \n",
       "         6                  Neutral  0.348305  \n",
       "         7                 Negative  0.460997  \n",
       "         8                 Negative  0.346679  \n",
       "         9                  Neutral  0.276451  \n",
       "         10           Very Positive  0.303925  \n",
       "         11           Very Positive  0.354398  \n",
       "         12           Very Negative  0.289577  \n",
       "roberta  0                 negative  0.633547  \n",
       "         1                  neutral  0.625573  \n",
       "         2                 positive  0.928684  \n",
       "         3                 positive  0.654007  \n",
       "         4                 negative  0.786693  \n",
       "         5                 negative  0.886169  \n",
       "         6                  neutral  0.523557  \n",
       "         7                 negative  0.899095  \n",
       "         8                 negative  0.685357  \n",
       "         9                  neutral  0.633503  \n",
       "         10                 neutral  0.612150  \n",
       "         11                 neutral  0.738948  \n",
       "         12                 neutral  0.804584  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_dict = print_sentiment_df(sentence_list, bert_sentiment_analyzer)\n",
    "roberta_dict = print_sentiment_df(sentence_list, roberta_sentiment_analyzer)\n",
    "bert_df = pd.DataFrame.from_records(data=next(iter(bert_dict.values())))\n",
    "roberta_df = pd.DataFrame.from_records(data=next(iter(roberta_dict.values())))\n",
    "result_df = pd.concat([bert_df, roberta_df], keys=[\"bert\", \"roberta\"], names=[\"llm_name\", \"sentence_id\"] ,axis=0)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f275ca-1bba-46cc-aa42-2e0f3d9ac268",
   "metadata": {},
   "source": [
    "<div style=\"color: white; display: block; border-radius: 5px; background-color: #09ba73; width: 100%; height: 80%; font-size: 110%; font-family: Verdana; letter-spacing: 0.5px;\">\n",
    "    <h3 style=\"padding: 10px; color: white;\">Result: You can compare results of Roberta and Bert model for sentiment analysis.</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d09f03-5746-46b7-809b-d7bd9d3ed351",
   "metadata": {},
   "source": [
    "<div style=\"color: white; display: block; border-radius: 5px; background-color: #09ba73; width: 100%; height: 80%; font-size: 110%; font-family: Verdana; letter-spacing: 0.5px;\">\n",
    "    <h3 style=\"padding: 10px; color: white;\">NER - Named Entity Recognition Example</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b325ed12-f455-4329-b48c-7dd0aab0b252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'B-LOC', 'score': 0.9997718, 'index': 9, 'word': 'Istanbul', 'start': 50, 'end': 58}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer, BertForTokenClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "example = \"I have been working on artificial intelligence in Istanbul since my student years.\"\n",
    "ner_results = nlp(example)\n",
    "print(ner_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d8446bdf-a0fd-45a5-9caa-1acd06146234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
