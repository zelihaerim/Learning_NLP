{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building A Chatbot\n",
    "In this video We'll go over an example of how to design and implement an LLM-powered chatbot. This chatbot will be able to have a conversation and remember previous interactions.\n",
    "\n",
    "Note that this chatbot that we build will only use the language model to have a conversation. There are several other related concepts that you may be looking for:\n",
    "\n",
    "- Conversational RAG: Enable a chatbot experience over an external source of data\n",
    "- Agents: Build a chatbot that can take actions\n",
    "\n",
    "This video tutorial will cover the basics which will be helpful for those two more advanced topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "your_api_key = \"\"\n",
    "groq_api_key = your_api_key\n",
    "groq_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x00000225ACF94CE0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000225ACF957C0>, model_name='Gemma2-9b-It', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model=ChatGroq(model=\"Gemma2-9b-It\",groq_api_key=groq_api_key)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Zeliha Erim, it's nice to meet you! \\n\\nBeing a Software Engineer is an exciting field. What kind of software engineering do you specialize in?  \\n\\nI'm always interested in learning more about what people do. 😊\\n\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "ai_message = model.invoke([HumanMessage(content=\"Hi , My name is Zeliha Erim and I am a Software Engineer\")])\n",
    "ai_message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are Zeliha Erim, and you are a Software Engineer!  \n",
      "\n",
      "I remembered that from our earlier conversation.  \n",
      "\n",
      "Is there anything else you'd like to talk about? Perhaps you could tell me about a project you're working on?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "ai_message_1 = model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi , My name is Zeliha Erim and I am a Software Engineer\"),\n",
    "        AIMessage(content= ai_message.content),\n",
    "        HumanMessage(content=\"Hey What's my name and what do I do?\")\n",
    "    ]\n",
    ")\n",
    "print(ai_message_1.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Message History\n",
    "We can use a Message History class to wrap our model and make it stateful. This will keep track of inputs and outputs of the model, and store them in some datastore. Future interactions will then load those messages and pass them into the chain as part of the input. Let's see how to use this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "store={}\n",
    "\"\"\"\n",
    "get_session_history –\n",
    "Function that returns a new BaseChatMessageHistory. \n",
    "This function should either take a single positional argument session_id of type string and return a corresponding\n",
    "chat message history instance. .. code-block:: python\n",
    "def get_session_history(\n",
    "session_id: str, *, user_id: Optional[str]=None\n",
    "\n",
    ") -> BaseChatMessageHistory:\n",
    "…\n",
    "\"\"\"\n",
    "def get_session_history(session_id:str)->BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id]=ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "config={\"configurable\":{\"session_id\":\"chat1\"}}\n",
    "parser = StrOutputParser()\n",
    "chain = model|parser\n",
    "with_message_history=RunnableWithMessageHistory(chain,  get_session_history=get_session_history, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m\n",
       "\u001b[0mChatMessageHistory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmessages\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlangchain_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBaseMessage\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "In memory implementation of chat message history.\n",
       "\n",
       "Stores messages in a memory list.\n",
       "\u001b[1;31mInit docstring:\u001b[0m\n",
       "Create a new model by parsing and validating input data from keyword arguments.\n",
       "\n",
       "Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n",
       "validated to form a valid model.\n",
       "\n",
       "`self` is explicitly positional-only to allow `self` as a field name.\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\monoz\\anaconda3\\envs\\torch_gpu\\lib\\site-packages\\langchain_core\\chat_history.py\n",
       "\u001b[1;31mType:\u001b[0m           ModelMetaclass\n",
       "\u001b[1;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?ChatMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m\n",
       "\u001b[0mRunnableWithMessageHistory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mrunnable\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Union[Runnable[Union[MessagesOrDictWithMessages], Union[str, BaseMessage, MessagesOrDictWithMessages]], LanguageModelLike]'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mget_session_history\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'GetSessionHistoryCallable'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0minput_messages_key\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Optional[str]'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0moutput_messages_key\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Optional[str]'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mhistory_messages_key\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Optional[str]'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mhistory_factory_config\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Optional[Sequence[ConfigurableFieldSpec]]'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mbound\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunnables\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRunnable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mInput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m+\u001b[0m\u001b[0mOutput\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtyping\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mconfig\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunnables\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRunnableConfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mconfig_factories\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtyping\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCallable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlangchain_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunnables\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRunnableConfig\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunnables\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRunnableConfig\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcustom_input_type\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcustom_output_type\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Runnable that manages chat message history for another Runnable.\n",
       "\n",
       "A chat message history is a sequence of messages that represent a conversation.\n",
       "\n",
       "RunnableWithMessageHistory wraps another Runnable and manages the chat message\n",
       "history for it; it is responsible for reading and updating the chat message\n",
       "history.\n",
       "\n",
       "The formats supported for the inputs and outputs of the wrapped Runnable\n",
       "are described below.\n",
       "\n",
       "RunnableWithMessageHistory must always be called with a config that contains\n",
       "the appropriate parameters for the chat message history factory.\n",
       "\n",
       "By default, the Runnable is expected to take a single configuration parameter\n",
       "called `session_id` which is a string. This parameter is used to create a new\n",
       "or look up an existing chat message history that matches the given session_id.\n",
       "\n",
       "In this case, the invocation would look like this:\n",
       "\n",
       "`with_history.invoke(..., config={\"configurable\": {\"session_id\": \"bar\"}})`\n",
       "; e.g., ``{\"configurable\": {\"session_id\": \"<SESSION_ID>\"}}``.\n",
       "\n",
       "The configuration can be customized by passing in a list of\n",
       "``ConfigurableFieldSpec`` objects to the ``history_factory_config`` parameter (see\n",
       "example below).\n",
       "\n",
       "In the examples, we will use a chat message history with an in-memory\n",
       "implementation to make it easy to experiment and see the results.\n",
       "\n",
       "For production use cases, you will want to use a persistent implementation\n",
       "of chat message history, such as ``RedisChatMessageHistory``.\n",
       "\n",
       "Parameters:\n",
       "    get_session_history: Function that returns a new BaseChatMessageHistory.\n",
       "        This function should either take a single positional argument\n",
       "        `session_id` of type string and return a corresponding\n",
       "        chat message history instance.\n",
       "    input_messages_key: Must be specified if the base runnable accepts a dict\n",
       "        as input. The key in the input dict that contains the messages.\n",
       "    output_messages_key: Must be specified if the base Runnable returns a dict\n",
       "        as output. The key in the output dict that contains the messages.\n",
       "    history_messages_key: Must be specified if the base runnable accepts a dict\n",
       "        as input and expects a separate key for historical messages.\n",
       "    history_factory_config: Configure fields that should be passed to the\n",
       "        chat history factory. See ``ConfigurableFieldSpec`` for more details.\n",
       "\n",
       "Example: Chat message history with an in-memory implementation for testing.\n",
       "\n",
       ".. code-block:: python\n",
       "\n",
       "    from operator import itemgetter\n",
       "    from typing import List\n",
       "\n",
       "    from langchain_openai.chat_models import ChatOpenAI\n",
       "\n",
       "    from langchain_core.chat_history import BaseChatMessageHistory\n",
       "    from langchain_core.documents import Document\n",
       "    from langchain_core.messages import BaseMessage, AIMessage\n",
       "    from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
       "    from pydantic import BaseModel, Field\n",
       "    from langchain_core.runnables import (\n",
       "        RunnableLambda,\n",
       "        ConfigurableFieldSpec,\n",
       "        RunnablePassthrough,\n",
       "    )\n",
       "    from langchain_core.runnables.history import RunnableWithMessageHistory\n",
       "\n",
       "\n",
       "    class InMemoryHistory(BaseChatMessageHistory, BaseModel):\n",
       "        \"\"\"In memory implementation of chat message history.\"\"\"\n",
       "\n",
       "        messages: List[BaseMessage] = Field(default_factory=list)\n",
       "\n",
       "        def add_messages(self, messages: List[BaseMessage]) -> None:\n",
       "            \"\"\"Add a list of messages to the store\"\"\"\n",
       "            self.messages.extend(messages)\n",
       "\n",
       "        def clear(self) -> None:\n",
       "            self.messages = []\n",
       "\n",
       "    # Here we use a global variable to store the chat message history.\n",
       "    # This will make it easier to inspect it to see the underlying results.\n",
       "    store = {}\n",
       "\n",
       "    def get_by_session_id(session_id: str) -> BaseChatMessageHistory:\n",
       "        if session_id not in store:\n",
       "            store[session_id] = InMemoryHistory()\n",
       "        return store[session_id]\n",
       "\n",
       "\n",
       "    history = get_by_session_id(\"1\")\n",
       "    history.add_message(AIMessage(content=\"hello\"))\n",
       "    print(store)  # noqa: T201\n",
       "\n",
       "\n",
       "Example where the wrapped Runnable takes a dictionary input:\n",
       "\n",
       "    .. code-block:: python\n",
       "\n",
       "        from typing import Optional\n",
       "\n",
       "        from langchain_community.chat_models import ChatAnthropic\n",
       "        from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
       "        from langchain_core.runnables.history import RunnableWithMessageHistory\n",
       "\n",
       "\n",
       "        prompt = ChatPromptTemplate.from_messages([\n",
       "            (\"system\", \"You're an assistant who's good at {ability}\"),\n",
       "            MessagesPlaceholder(variable_name=\"history\"),\n",
       "            (\"human\", \"{question}\"),\n",
       "        ])\n",
       "\n",
       "        chain = prompt | ChatAnthropic(model=\"claude-2\")\n",
       "\n",
       "        chain_with_history = RunnableWithMessageHistory(\n",
       "            chain,\n",
       "            # Uses the get_by_session_id function defined in the example\n",
       "            # above.\n",
       "            get_by_session_id,\n",
       "            input_messages_key=\"question\",\n",
       "            history_messages_key=\"history\",\n",
       "        )\n",
       "\n",
       "        print(chain_with_history.invoke(  # noqa: T201\n",
       "            {\"ability\": \"math\", \"question\": \"What does cosine mean?\"},\n",
       "            config={\"configurable\": {\"session_id\": \"foo\"}}\n",
       "        ))\n",
       "\n",
       "        # Uses the store defined in the example above.\n",
       "        print(store)  # noqa: T201\n",
       "\n",
       "        print(chain_with_history.invoke(  # noqa: T201\n",
       "            {\"ability\": \"math\", \"question\": \"What's its inverse\"},\n",
       "            config={\"configurable\": {\"session_id\": \"foo\"}}\n",
       "        ))\n",
       "\n",
       "        print(store)  # noqa: T201\n",
       "\n",
       "\n",
       "Example where the session factory takes two keys, user_id and conversation id):\n",
       "\n",
       "    .. code-block:: python\n",
       "\n",
       "        store = {}\n",
       "\n",
       "        def get_session_history(\n",
       "            user_id: str, conversation_id: str\n",
       "        ) -> BaseChatMessageHistory:\n",
       "            if (user_id, conversation_id) not in store:\n",
       "                store[(user_id, conversation_id)] = InMemoryHistory()\n",
       "            return store[(user_id, conversation_id)]\n",
       "\n",
       "        prompt = ChatPromptTemplate.from_messages([\n",
       "            (\"system\", \"You're an assistant who's good at {ability}\"),\n",
       "            MessagesPlaceholder(variable_name=\"history\"),\n",
       "            (\"human\", \"{question}\"),\n",
       "        ])\n",
       "\n",
       "        chain = prompt | ChatAnthropic(model=\"claude-2\")\n",
       "\n",
       "        with_message_history = RunnableWithMessageHistory(\n",
       "            chain,\n",
       "            get_session_history=get_session_history,\n",
       "            input_messages_key=\"question\",\n",
       "            history_messages_key=\"history\",\n",
       "            history_factory_config=[\n",
       "                ConfigurableFieldSpec(\n",
       "                    id=\"user_id\",\n",
       "                    annotation=str,\n",
       "                    name=\"User ID\",\n",
       "                    description=\"Unique identifier for the user.\",\n",
       "                    default=\"\",\n",
       "                    is_shared=True,\n",
       "                ),\n",
       "                ConfigurableFieldSpec(\n",
       "                    id=\"conversation_id\",\n",
       "                    annotation=str,\n",
       "                    name=\"Conversation ID\",\n",
       "                    description=\"Unique identifier for the conversation.\",\n",
       "                    default=\"\",\n",
       "                    is_shared=True,\n",
       "                ),\n",
       "            ],\n",
       "        )\n",
       "\n",
       "        with_message_history.invoke(\n",
       "            {\"ability\": \"math\", \"question\": \"What does cosine mean?\"},\n",
       "            config={\"configurable\": {\"user_id\": \"123\", \"conversation_id\": \"1\"}}\n",
       "        )\n",
       "\u001b[1;31mInit docstring:\u001b[0m\n",
       "Initialize RunnableWithMessageHistory.\n",
       "\n",
       "Args:\n",
       "    runnable: The base Runnable to be wrapped. Must take as input one of:\n",
       "        1. A sequence of BaseMessages\n",
       "        2. A dict with one key for all messages\n",
       "        3. A dict with one key for the current input string/message(s) and\n",
       "            a separate key for historical messages. If the input key points\n",
       "            to a string, it will be treated as a HumanMessage in history.\n",
       "\n",
       "        Must return as output one of:\n",
       "        1. A string which can be treated as an AIMessage\n",
       "        2. A BaseMessage or sequence of BaseMessages\n",
       "        3. A dict with a key for a BaseMessage or sequence of BaseMessages\n",
       "\n",
       "    get_session_history: Function that returns a new BaseChatMessageHistory.\n",
       "        This function should either take a single positional argument\n",
       "        `session_id` of type string and return a corresponding\n",
       "        chat message history instance.\n",
       "        .. code-block:: python\n",
       "\n",
       "            def get_session_history(\n",
       "                session_id: str,\n",
       "                *,\n",
       "                user_id: Optional[str]=None\n",
       "            ) -> BaseChatMessageHistory:\n",
       "              ...\n",
       "\n",
       "        Or it should take keyword arguments that match the keys of\n",
       "        `session_history_config_specs` and return a corresponding\n",
       "        chat message history instance.\n",
       "\n",
       "        .. code-block:: python\n",
       "\n",
       "            def get_session_history(\n",
       "                *,\n",
       "                user_id: str,\n",
       "                thread_id: str,\n",
       "            ) -> BaseChatMessageHistory:\n",
       "                ...\n",
       "\n",
       "    input_messages_key: Must be specified if the base runnable accepts a dict\n",
       "        as input. Default is None.\n",
       "    output_messages_key: Must be specified if the base runnable returns a dict\n",
       "        as output. Default is None.\n",
       "    history_messages_key: Must be specified if the base runnable accepts a dict\n",
       "        as input and expects a separate key for historical messages.\n",
       "    history_factory_config: Configure fields that should be passed to the\n",
       "        chat history factory. See ``ConfigurableFieldSpec`` for more details.\n",
       "        Specifying these allows you to pass multiple config keys\n",
       "        into the get_session_history factory.\n",
       "    **kwargs: Arbitrary additional kwargs to pass to parent class\n",
       "        ``RunnableBindingBase`` init.\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\monoz\\anaconda3\\envs\\torch_gpu\\lib\\site-packages\\langchain_core\\runnables\\history.py\n",
       "\u001b[1;31mType:\u001b[0m           ModelMetaclass\n",
       "\u001b[1;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `RunnableConfig` not found.\n"
     ]
    }
   ],
   "source": [
    "?RunnableConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Zeliha Erim! It's nice to meet you.\\n\\nAs a large language model, I don't have personal experiences like being a software engineer, but I can access and process information about it. \\n\\nIs there anything specific you'd like to talk about related to software engineering? I can help with:\\n\\n* **Explaining concepts:** If you have a question about a particular programming language, technology, or software development process, I can try my best to explain it.\\n* **Generating code:** I can generate code snippets in various programming languages, though it's always important to review and test the code before using it.\\n* **Brainstorming ideas:** If you're stuck on a problem or need help coming up with new ideas, I can offer suggestions based on my knowledge.\\n\\nLet me know how I can be of assistance!\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content = \"Hi , My name is Zeliha Erim and I am a Software Engineer\")],\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Zeliha Erim.  \\n\\nYou told me at the beginning of our conversation! 😊  \\n\\n\\n\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Does model remember me?\n",
    "with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my name?\")],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"That's great to know, Zeliha! İstanbul is a vibrant and historic city.  Are there any particular places or things you enjoy about living there?  \\n\\nPerhaps you'd like to talk about your favorite local restaurants, historical sites, or maybe even the bustling atmosphere of the city? I'm happy to chat about anything you'd like. 🌎  \\n\\n\\n\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content = \"I am living in İstanbul.\")]\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Me too! Chocolate is delicious. 🍫\\n\\nDo you have a favorite kind of chocolate?  Dark, milk, white?  Or maybe something more unique, like Turkish delight?  \\n\\nI'm always curious to learn about people's chocolate preferences. 😋\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content = \"I love eating chocolate.\")]\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"That's fantastic, Zeliha! NLP is a fascinating field.  \\n\\nWhat aspects of NLP are you most interested in?  Text generation, machine translation, sentiment analysis?  I'm eager to hear more about your studies.  \\n\\nAnd maybe you can even put my own NLP abilities to the test! 😉  \\n\\n\\n\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content = \"I am studying NLP.\")]\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"That's great! Did you find everything you needed? \\n\\nI always find grocery shopping interesting, especially when trying to find new and unique ingredients.  \\n\\nDid you discover anything new or exciting at the store?  Perhaps a special type of chocolate?  🍫  \\n\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content = \"I went to grocery yesterday.\")]\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Blue is a wonderful color!  It's calming, versatile, and often associated with the sky and ocean.  \\n\\nIs there a particular shade of blue that you love the most?  Perhaps a deep navy, a vibrant turquoise, or a soft sky blue?  \\n\\nI'm always curious about people's color preferences. 💙\\n\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content = \"My favorite color is blue.\")]\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As an AI, I have no memory of past conversations and do not know your name. If you'd like to tell me, I'm happy to learn it! 😊\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run chat with different session id\n",
    "## change the config-->session id\n",
    "config1={\"configurable\":{\"session_id\":\"chat2\"}}\n",
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Whats my name\")],\n",
    "    config=config1\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Bulut, it's nice to meet you!  \\n\\nIs there anything I can help you with today?\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hey My name is Bulut\")],\n",
    "    config=config1\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Bulut!  I remember now.  😊  \\n\\nDo you want to chat about something, Bulut?  Or would you like me to help you with a task?\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Whats my name\")],\n",
    "    config=config1\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "store : {'chat1': InMemoryChatMessageHistory(messages=[HumanMessage(content='Hi , My name is Zeliha Erim and I am a Software Engineer', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hello Zeliha Erim! It's nice to meet you.\\n\\nAs a large language model, I don't have personal experiences like being a software engineer, but I can access and process information about it. \\n\\nIs there anything specific you'd like to talk about related to software engineering? I can help with:\\n\\n* **Explaining concepts:** If you have a question about a particular programming language, technology, or software development process, I can try my best to explain it.\\n* **Generating code:** I can generate code snippets in various programming languages, though it's always important to review and test the code before using it.\\n* **Brainstorming ideas:** If you're stuck on a problem or need help coming up with new ideas, I can offer suggestions based on my knowledge.\\n\\nLet me know how I can be of assistance!\\n\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"What's my name?\", additional_kwargs={}, response_metadata={}), AIMessage(content='Your name is Zeliha Erim.  \\n\\nYou told me at the beginning of our conversation! 😊  \\n\\n\\n\\n', additional_kwargs={}, response_metadata={}), HumanMessage(content='I am living in İstanbul.', additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's great to know, Zeliha! İstanbul is a vibrant and historic city.  Are there any particular places or things you enjoy about living there?  \\n\\nPerhaps you'd like to talk about your favorite local restaurants, historical sites, or maybe even the bustling atmosphere of the city? I'm happy to chat about anything you'd like. 🌎  \\n\\n\\n\\n\", additional_kwargs={}, response_metadata={}), HumanMessage(content='I love eating chocolate.', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Me too! Chocolate is delicious. 🍫\\n\\nDo you have a favorite kind of chocolate?  Dark, milk, white?  Or maybe something more unique, like Turkish delight?  \\n\\nI'm always curious to learn about people's chocolate preferences. 😋\\n\", additional_kwargs={}, response_metadata={}), HumanMessage(content='I am studying NLP.', additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's fantastic, Zeliha! NLP is a fascinating field.  \\n\\nWhat aspects of NLP are you most interested in?  Text generation, machine translation, sentiment analysis?  I'm eager to hear more about your studies.  \\n\\nAnd maybe you can even put my own NLP abilities to the test! 😉  \\n\\n\\n\\n\", additional_kwargs={}, response_metadata={}), HumanMessage(content='I went to grocery yesterday.', additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's great! Did you find everything you needed? \\n\\nI always find grocery shopping interesting, especially when trying to find new and unique ingredients.  \\n\\nDid you discover anything new or exciting at the store?  Perhaps a special type of chocolate?  🍫  \\n\\n\\n\\n\\n\", additional_kwargs={}, response_metadata={}), HumanMessage(content='My favorite color is blue.', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Blue is a wonderful color!  It's calming, versatile, and often associated with the sky and ocean.  \\n\\nIs there a particular shade of blue that you love the most?  Perhaps a deep navy, a vibrant turquoise, or a soft sky blue?  \\n\\nI'm always curious about people's color preferences. 💙\\n\\n\\n\\n\\n\", additional_kwargs={}, response_metadata={})]), 'chat2': InMemoryChatMessageHistory(messages=[HumanMessage(content='Whats my name', additional_kwargs={}, response_metadata={}), AIMessage(content=\"As an AI, I have no memory of past conversations and do not know your name. If you'd like to tell me, I'm happy to learn it! 😊\\n\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Hey My name is Bulut', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hi Bulut, it's nice to meet you!  \\n\\nIs there anything I can help you with today?\\n\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Whats my name', additional_kwargs={}, response_metadata={}), AIMessage(content='Your name is Bulut!  I remember now.  😊  \\n\\nDo you want to chat about something, Bulut?  Or would you like me to help you with a task?\\n', additional_kwargs={}, response_metadata={})])}\n"
     ]
    }
   ],
   "source": [
    "print(f\"store : {store}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hi , My name is Zeliha Erim and I am a Software Engineer', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Hello Zeliha Erim! It's nice to meet you.\\n\\nAs a large language model, I don't have personal experiences like being a software engineer, but I can access and process information about it. \\n\\nIs there anything specific you'd like to talk about related to software engineering? I can help with:\\n\\n* **Explaining concepts:** If you have a question about a particular programming language, technology, or software development process, I can try my best to explain it.\\n* **Generating code:** I can generate code snippets in various programming languages, though it's always important to review and test the code before using it.\\n* **Brainstorming ideas:** If you're stuck on a problem or need help coming up with new ideas, I can offer suggestions based on my knowledge.\\n\\nLet me know how I can be of assistance!\\n\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"What's my name?\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Your name is Zeliha Erim.  \\n\\nYou told me at the beginning of our conversation! 😊  \\n\\n\\n\\n', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I am living in İstanbul.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"That's great to know, Zeliha! İstanbul is a vibrant and historic city.  Are there any particular places or things you enjoy about living there?  \\n\\nPerhaps you'd like to talk about your favorite local restaurants, historical sites, or maybe even the bustling atmosphere of the city? I'm happy to chat about anything you'd like. 🌎  \\n\\n\\n\\n\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I love eating chocolate.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Me too! Chocolate is delicious. 🍫\\n\\nDo you have a favorite kind of chocolate?  Dark, milk, white?  Or maybe something more unique, like Turkish delight?  \\n\\nI'm always curious to learn about people's chocolate preferences. 😋\\n\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I am studying NLP.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"That's fantastic, Zeliha! NLP is a fascinating field.  \\n\\nWhat aspects of NLP are you most interested in?  Text generation, machine translation, sentiment analysis?  I'm eager to hear more about your studies.  \\n\\nAnd maybe you can even put my own NLP abilities to the test! 😉  \\n\\n\\n\\n\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I went to grocery yesterday.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"That's great! Did you find everything you needed? \\n\\nI always find grocery shopping interesting, especially when trying to find new and unique ingredients.  \\n\\nDid you discover anything new or exciting at the store?  Perhaps a special type of chocolate?  🍫  \\n\\n\\n\\n\\n\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='My favorite color is blue.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Blue is a wonderful color!  It's calming, versatile, and often associated with the sky and ocean.  \\n\\nIs there a particular shade of blue that you love the most?  Perhaps a deep navy, a vibrant turquoise, or a soft sky blue?  \\n\\nI'm always curious about people's color preferences. 💙\\n\\n\\n\\n\\n\", additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store[\"chat1\"].messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat id : chat1\n",
      "History : Human: Hi , My name is Zeliha Erim and I am a Software Engineer\n",
      "AI: Hello Zeliha Erim! It's nice to meet you.\n",
      "\n",
      "As a large language model, I don't have personal experiences like being a software engineer, but I can access and process information about it. \n",
      "\n",
      "Is there anything specific you'd like to talk about related to software engineering? I can help with:\n",
      "\n",
      "* **Explaining concepts:** If you have a question about a particular programming language, technology, or software development process, I can try my best to explain it.\n",
      "* **Generating code:** I can generate code snippets in various programming languages, though it's always important to review and test the code before using it.\n",
      "* **Brainstorming ideas:** If you're stuck on a problem or need help coming up with new ideas, I can offer suggestions based on my knowledge.\n",
      "\n",
      "Let me know how I can be of assistance!\n",
      "\n",
      "Human: What's my name?\n",
      "AI: Your name is Zeliha Erim.  \n",
      "\n",
      "You told me at the beginning of our conversation! 😊  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Human: I am living in İstanbul.\n",
      "AI: That's great to know, Zeliha! İstanbul is a vibrant and historic city.  Are there any particular places or things you enjoy about living there?  \n",
      "\n",
      "Perhaps you'd like to talk about your favorite local restaurants, historical sites, or maybe even the bustling atmosphere of the city? I'm happy to chat about anything you'd like. 🌎  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Human: I love eating chocolate.\n",
      "AI: Me too! Chocolate is delicious. 🍫\n",
      "\n",
      "Do you have a favorite kind of chocolate?  Dark, milk, white?  Or maybe something more unique, like Turkish delight?  \n",
      "\n",
      "I'm always curious to learn about people's chocolate preferences. 😋\n",
      "\n",
      "Human: I am studying NLP.\n",
      "AI: That's fantastic, Zeliha! NLP is a fascinating field.  \n",
      "\n",
      "What aspects of NLP are you most interested in?  Text generation, machine translation, sentiment analysis?  I'm eager to hear more about your studies.  \n",
      "\n",
      "And maybe you can even put my own NLP abilities to the test! 😉  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Human: I went to grocery yesterday.\n",
      "AI: That's great! Did you find everything you needed? \n",
      "\n",
      "I always find grocery shopping interesting, especially when trying to find new and unique ingredients.  \n",
      "\n",
      "Did you discover anything new or exciting at the store?  Perhaps a special type of chocolate?  🍫  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Human: My favorite color is blue.\n",
      "AI: Blue is a wonderful color!  It's calming, versatile, and often associated with the sky and ocean.  \n",
      "\n",
      "Is there a particular shade of blue that you love the most?  Perhaps a deep navy, a vibrant turquoise, or a soft sky blue?  \n",
      "\n",
      "I'm always curious about people's color preferences. 💙\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "chat id : chat2\n",
      "History : Human: Whats my name\n",
      "AI: As an AI, I have no memory of past conversations and do not know your name. If you'd like to tell me, I'm happy to learn it! 😊\n",
      "\n",
      "Human: Hey My name is Bulut\n",
      "AI: Hi Bulut, it's nice to meet you!  \n",
      "\n",
      "Is there anything I can help you with today?\n",
      "\n",
      "Human: Whats my name\n",
      "AI: Your name is Bulut!  I remember now.  😊  \n",
      "\n",
      "Do you want to chat about something, Bulut?  Or would you like me to help you with a task?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key, value in store.items():\n",
    "    print(f\"chat id : {key}\\nHistory : {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt templates\n",
    "Prompt Templates help to turn raw user information into a format that the LLM can work with. In this case, the raw user input is just a message, which we are passing to the LLM. Let's now make that a bit more complicated. First, let's add in a system message with some custom instructions (but still taking messages as input). Next, we'll add in more input besides just the messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test chain: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hi Zeliha! Welcome to the world of NLP. \\n\\nWhat can I help you with today? As a senior NLP engineer, I'm here to answer your questions and guide you as you learn. Don't hesitate to ask anything, even if it seems basic. \\n\\nWhat are you working on right now? 😄  \\n\\n\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are a NLP software engineer and answers questions of junior software engineer. Answer the question based on this description.\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "chain=prompt|model\n",
    "with_message_history=RunnableWithMessageHistory(chain, get_session_history)\n",
    "print(\"Test chain: \\n\")\n",
    "human_message = \"Hi My name is Zeliha I am a software engineer.\"\n",
    "response = chain.invoke({\"messages\":[HumanMessage(content=human_message)]})\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Zeliha!  \\n\\nWelcome to the world of NLP! I'm here to help you out. What can I do for you today? \\n\\nIs there something specific about NLP you'd like to know more about, or are you working on a project that you need help with?  \\n\\nDon't hesitate to ask anything, even if you think it's a basic question. 😊 \\n\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"chat3\"}}\n",
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=human_message)],\n",
    "    config=config\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Zeliha! 😊  \\n\\nNice to officially meet you, Zeliha.  What can I help you with today?\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke([HumanMessage(content=\"What's my name?\")], config=config)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You're smart to ask about NLP pipelines, Zeliha! They're a super important part of how we build and use NLP applications.  \\n\\nThink of an NLP pipeline like an assembly line for natural language processing.  Instead of cars, we're processing text! \\n\\nHere's why engineers use them:\\n\\n* **Structure and Organization:** Pipelines break down complex NLP tasks into smaller, manageable steps. This makes the code easier to understand, debug, and maintain.\\n\\n* **Modularity:** Each step in the pipeline is a separate component that can be easily swapped out or customized.  Need to try a different tokenizer? No problem! Just replace the tokenization step in your pipeline.\\n* **Efficiency:** Pipelines can be optimized to process text quickly and efficiently.  \\n\\n**Here's a simplified example of an NLP pipeline:**\\n\\n1. **Tokenization:** Breaking down text into individual words or units (tokens).\\n\\n2. **Part-of-Speech Tagging:** Identifying the grammatical role of each word (noun, verb, adjective, etc.).\\n\\n3. **Named Entity Recognition:**  Finding and classifying named entities like people, organizations, locations, etc.\\n\\n4. **Sentiment Analysis:** Determining the overall emotional tone of the text (positive, negative, neutral).\\n\\n**Popular NLP Pipeline Libraries:**\\n\\n* **SpaCy:**  A fast and efficient library with pre-trained models for many common NLP tasks.\\n* **HuggingFace Transformers:**  Provides access to a vast library of state-of-the-art transformer models.\\n\\nLet me know if you have any other questions about NLP pipelines, Zeliha!\\n\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke([HumanMessage(content=\"Could you tell me about why engineers using NLP pipeline?\")], config=config)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Merhaba Zeliha,\\n\\nBenimle konuşmak için çok mutluyum! Bir yazılım mühendisi olduğunu duyduğum için heyecanlıyım. Bana nasıl yardımcı olabilirim? 😊  \\n\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create settings and other stuffs\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "chain = prompt|model|parser\n",
    "with_message_history=RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\"\n",
    ")\n",
    "config = {\"configurable\": {\"session_id\": \"chat4\"}}\n",
    "# invoke messages\n",
    "repsonse=with_message_history.invoke(\n",
    "    {'messages': [HumanMessage(content=\"Hi My name is Zeliha I am a software engineer.\")],\"language\":\"Türkçe\"},\n",
    "    config=config\n",
    ")\n",
    "repsonse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Evet, elbette! Adın Zeliha olduğunu hatırlıyorum.  \\n\\nNasıl yardımcı olabilirim? 😊 \\n\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do you remember me?\n",
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Do you remeber me, what is my name?\")], \"language\": \"Türkçe\"},\n",
    "    config=config,\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Managing the Conversation History\n",
    "One important concept to understand when building chatbots is how to manage conversation history. If left unmanaged, the list of messages will grow unbounded and potentially overflow the context window of the LLM. Therefore, it is important to add a step that limits the size of the messages you are passing in.\n",
    "'trim_messages' helper to reduce how many messages we're sending to the model. The trimmer allows us to specify how many tokens we want to keep, along with other parameters like if we want to always keep the system message and whether to allow partial messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage,trim_messages\n",
    "trimmer=trim_messages(\n",
    "    max_tokens=45,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\"\n",
    ")\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter # for trimmer\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain=(\n",
    "    RunnablePassthrough.assign(messages=itemgetter(\"messages\")|trimmer) # trim process\n",
    "    | prompt\n",
    "    | model\n",
    ")\n",
    "response=chain.invoke(\n",
    "    {\n",
    "    \"messages\":messages + [HumanMessage(content=\"What ice cream do i like\")],\n",
    "    \"language\":\"English\"\n",
    "    }\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"what math problem did i ask\")],\n",
    "        \"language\": \"English\",\n",
    "    }\n",
    ")\n",
    "response.content\n",
    "# it reached very early conversation because it is trimmed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets wrap this in the Message History\n",
    "config={\"configurable\":{\"session_id\":\"chat5\"}}\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\",\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"whats my name?\")],\n",
    "        \"language\": \"English\",\n",
    "    }\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"what math problem did i ask?\")],\n",
    "        \"language\": \"English\",\n",
    "    }\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chat1 history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='I am studying NLP.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"That's fantastic, Zeliha! NLP is a fascinating field.  \\n\\nWhat aspects of NLP are you most interested in?  Text generation, machine translation, sentiment analysis?  I'm eager to hear more about your studies.  \\n\\nAnd maybe you can even put my own NLP abilities to the test! 😉  \\n\\n\\n\\n\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I went to grocery yesterday.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"That's great! Did you find everything you needed? \\n\\nI always find grocery shopping interesting, especially when trying to find new and unique ingredients.  \\n\\nDid you discover anything new or exciting at the store?  Perhaps a special type of chocolate?  🍫  \\n\\n\\n\\n\\n\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='My favorite color is blue.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Blue is a wonderful color!  It's calming, versatile, and often associated with the sky and ocean.  \\n\\nIs there a particular shade of blue that you love the most?  Perhaps a deep navy, a vibrant turquoise, or a soft sky blue?  \\n\\nI'm always curious about people's color preferences. 💙\\n\\n\\n\\n\\n\", additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trime chat1 session id history\n",
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "trimmer=trim_messages(\n",
    "    max_tokens=300,\n",
    "    strategy=\"last\", # first\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\"\n",
    ")\n",
    "trimmer.invoke(store[\"chat1\"].messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hi , My name is Zeliha Erim and I am a Software Engineer', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Hello Zeliha Erim! It's nice to meet you.\\n\\nAs a large language model, I don't have personal experiences like being a software engineer, but I can access and process information about it. \\n\\nIs there anything specific you'd like to talk about related to software engineering? I can help with:\\n\\n* **Explaining concepts:** If you have a question about a particular programming language, technology, or software development process, I can try my best to explain it.\\n* **Generating code:** I can generate code snippets in various programming languages, though it's always important to review and test the code before using it.\\n* **Brainstorming ideas:** If you're stuck on a problem or need help coming up with new ideas, I can offer suggestions based on my knowledge.\\n\\nLet me know how I can be of assistance!\\n\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"What's my name?\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Your name is Zeliha Erim.  \\n\\nYou told me at the beginning of our conversation! 😊  \\n\\n\\n\\n', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I am living in İstanbul.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"That's great to know, Zeliha! İstanbul is a vibrant and historic city.  Are there any particular places or things you enjoy about living there?  \\n\\nPerhaps you'd like to talk about your favorite local restaurants, historical sites, or maybe even the bustling atmosphere of the city? I'm happy to chat about anything you'd like. 🌎  \\n\\n\\n\\n\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I love eating chocolate.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Me too! Chocolate is delicious. 🍫\\n\\nDo you have a favorite kind of chocolate?  Dark, milk, white?  Or maybe something more unique, like Turkish delight?  \\n\\nI'm always curious to learn about people's chocolate preferences. 😋\\n\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I am studying NLP.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"That's fantastic, Zeliha! NLP is a fascinating field.  \\n\\nWhat aspects of NLP are you most interested in?  Text generation, machine translation, sentiment analysis?  I'm eager to hear more about your studies.  \\n\\nAnd maybe you can even put my own NLP abilities to the test! 😉  \\n\\n\\n\\n\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I went to grocery yesterday.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"That's great! Did you find everything you needed? \\n\\nI always find grocery shopping interesting, especially when trying to find new and unique ingredients.  \\n\\nDid you discover anything new or exciting at the store?  Perhaps a special type of chocolate?  🍫  \\n\\n\\n\\n\\n\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='My favorite color is blue.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Blue is a wonderful color!  It's calming, versatile, and often associated with the sky and ocean.  \\n\\nIs there a particular shade of blue that you love the most?  Perhaps a deep navy, a vibrant turquoise, or a soft sky blue?  \\n\\nI'm always curious about people's color preferences. 💙\\n\\n\\n\\n\\n\", additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trime chat1 session id history\n",
    "trimmer=trim_messages(\n",
    "    max_tokens=1000,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\"\n",
    ")\n",
    "trimmer.invoke(store[\"chat1\"].messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As your assistant, I don't have access to personal information like your address or location.  \\n\\nIs there anything else I can help you with?  Perhaps you'd like to discuss your favorite things about blue, or maybe you have a question about NLP? 😊  \\n\\n\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter # for trimmer\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "trimmer=trim_messages(\n",
    "    max_tokens=300,\n",
    "    strategy=\"last\", # first\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\"\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are Zeliha's assistant. Answer all questions to the best of your ability in {language}.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "past_chat = store[\"chat1\"].messages\n",
    "chain=RunnablePassthrough.assign(messages=itemgetter(\"messages\")|trimmer)|prompt|model|parser\n",
    "# ask something about early questions\n",
    "response=chain.invoke(\n",
    "    {\n",
    "    \"messages\" : past_chat + [HumanMessage(content=\"Where do I live?\")],\n",
    "    \"language\" : \"English\"\n",
    "    }\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You told me your favorite color is blue!  💙 \\n\\nIs there something else you'd like to talk about? 😊  \\n\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ask something about latest questions\n",
    "response=chain.invoke(\n",
    "    {\n",
    "    \"messages\" : past_chat + [HumanMessage(content=\"What is my favoriete color?\")],\n",
    "    \"language\" : \"English\"\n",
    "    }\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
